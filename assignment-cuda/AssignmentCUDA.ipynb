{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "38X-ZkMbxLEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8QnTGD-ifiE",
        "outputId": "b0fc9865-8d4e-4bc7-c260-1c9bb9c2f51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 8abd1bed502a 6.1.85+ #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n",
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ],
      "source": [
        "!uname -a && cat /etc/*release"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RSxDC6LiwR2",
        "outputId": "3520bee0-5ab0-4ab5-a5a8-e74d07566f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 32\n",
            "drwxr-xr-x 1 root root  4096 Nov 23 10:24 .\n",
            "drwxr-xr-x 1 root root  4096 Nov 23 10:23 ..\n",
            "drwxr-xr-x 4 root root  4096 Nov 21 14:25 .config\n",
            "-rw-r--r-- 1 root root 14196 Nov 23 10:24 devicequery.zip\n",
            "drwxr-xr-x 1 root root  4096 Nov 21 14:25 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07P9-HQYiy0Q",
        "outputId": "dcb1e2bd-df06-415f-9c61-8ca36eab4e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 23 10:24:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip devicequery.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETryFAcji0Vm",
        "outputId": "4dfb5d76-ee5e-4796-ac47-14394a2fce04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  devicequery.zip\n",
            "  inflating: devicequery.cu          \n",
            "  inflating: helper_cuda.h           \n",
            "  inflating: helper_string.h         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc devicequery.cu -o devicequery"
      ],
      "metadata": {
        "id": "CkV0bmUWi6w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/devicequery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncIE_P3WjM1z",
        "outputId": "f93c8116-7c73-4238-fa10-5ced1207ceb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/devicequery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.2, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelized version with usage of Shared Memory"
      ],
      "metadata": {
        "id": "c3mH-KheLQXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code=r'''#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#ifdef __NVCC__\n",
        "#include <cuda.h>\n",
        "#endif\n",
        "#include <sys/time.h>\n",
        "\n",
        "// Simple define to index into a 1D array from 2D space\n",
        "#define I2D(num, c, r) ((r)*(num)+(c))\n",
        "\n",
        "/*\n",
        " * `step_kernel_mod` is currently a direct copy of the CPU reference solution\n",
        " * `step_kernel_ref` below. Accelerate it to run as a CUDA kernel.\n",
        " */\n",
        "\n",
        "#ifndef BLOCK_SIZE\n",
        "#define BLOCK_SIZE 16\n",
        "#endif\n",
        "#define BLOCK_WIDTH BLOCK_SIZE\n",
        "#define BLOCK_HEIGHT BLOCK_SIZE\n",
        "#ifndef SIZE\n",
        "#define SIZE 1000\n",
        "#endif\n",
        "#ifndef STEPS\n",
        "#define STEPS 200\n",
        "#endif\n",
        "\n",
        "#ifdef __NVCC__\n",
        "#ifndef SHARED\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "\n",
        "  /*\n",
        "    COMMENTS: Remember about coalescence (cudaMallocPitch())\n",
        "    remeber balancing between L1 and Shared Mem cudaDeviceSetCacheConfig\n",
        "    __shared__ to access shared memory\n",
        "    __constant__ to set something on constant memory\n",
        "  */\n",
        "  //int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "  int x = threadIdx.x + blockIdx.x * blockDim.x + 1;\n",
        "  int y = threadIdx.y + blockIdx.y * blockDim.y + 1;\n",
        "\n",
        "  if(x >= ni-1 || y >= nj-1) return;\n",
        "\n",
        "  // evaluate derivatives\n",
        "\n",
        "  d2tdx2 = temp_in[I2D(ni, x - 1, y)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x + 1, y)];\n",
        "  d2tdy2 = temp_in[I2D(ni, x, y - 1)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  // update temperatures\n",
        "  temp_out[I2D(ni, x, y)] = temp_in[I2D(ni, x, y)] /*sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)] */ + fact * (d2tdx2 + d2tdy2);\n",
        "\n",
        "}\n",
        "#else\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "\n",
        "  /*\n",
        "    COMMENTS: Remember about coalescence (cudaMallocPitch())\n",
        "    remeber balancing between L1 and Shared Mem cudaDeviceSetCacheConfig\n",
        "    __shared__ to access shared memory\n",
        "    __constant__ to set something on constant memory\n",
        "  */\n",
        "  //int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "  __shared__ float sharedMem[(BLOCK_WIDTH + 2) * (BLOCK_HEIGHT + 2)];\n",
        "\n",
        "  int x = threadIdx.x + blockIdx.x * blockDim.x + 1;\n",
        "  int y = threadIdx.y + blockIdx.y * blockDim.y + 1;\n",
        "\n",
        "  if(x >= ni-1 || y >= nj-1) return;\n",
        "\n",
        "  sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)] = temp_in[I2D(ni, x, y)];\n",
        "\n",
        "// TODO: put most of this inside the same warp\n",
        "\n",
        "  if(threadIdx.x == 0) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x, threadIdx.y + 1)] = temp_in[I2D(ni, x - 1, y)];\n",
        "  else if(threadIdx.x == blockDim.x - 1 || x == ni - 2) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 2, threadIdx.y + 1)] = temp_in[I2D(ni, x + 1, y)];\n",
        "\n",
        "  if(threadIdx.y == 0) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y)] = temp_in[I2D(ni, x, y - 1)];\n",
        "  else if(threadIdx.y == blockDim.y - 1 || y == nj - 2) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 2)] = temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  // evaluate derivatives\n",
        "  d2tdx2 = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x, threadIdx.y + 1)]\n",
        "    - 2 * sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)]\n",
        "    + sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 2, threadIdx.y + 1)];\n",
        "\n",
        "  d2tdy2 = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y)]\n",
        "    - 2 * sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)]\n",
        "    + sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 2)];\n",
        "\n",
        "    //d2tdx2 = temp_in[I2D(ni, x - 1, y)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x + 1, y)];\n",
        "    //d2tdy2 = temp_in[I2D(ni, x, y - 1)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  // update temperatures\n",
        "  temp_out[I2D(ni, x, y)] = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)] + fact * (d2tdx2 + d2tdy2);\n",
        "\n",
        "}\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  for ( int j=1; j < nj-1; j++ ) {\n",
        "    for ( int i=1; i < ni-1; i++ ) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  struct timeval stop, start;\n",
        "  int istep;\n",
        "  int nstep = STEPS; // number of time steps\n",
        "\n",
        "  // Specify our 2D dimensions\n",
        "  const int ni = SIZE;\n",
        "  const int nj = SIZE;\n",
        "  float tfac = 8.418e-5; // thermal diffusivity of silver\n",
        "\n",
        "  float *temp1_ref, *temp2_ref, *temp1, *temp2, *temp_tmp;\n",
        "\n",
        "  const int size = ni * nj * sizeof(float);\n",
        "\n",
        "  temp1_ref = (float*)malloc(size);\n",
        "  temp2_ref = (float*)malloc(size);\n",
        "  temp1 = (float*)malloc(size);\n",
        "  temp2 = (float*)malloc(size);\n",
        "\n",
        "  // Initialize with random data\n",
        "  for( int i = 0; i < ni*nj; ++i) {\n",
        "    temp1_ref[i] = temp2_ref[i] = temp1[i] = temp2[i] = (float)rand()/(float)(RAND_MAX/100.0f);\n",
        "  }\n",
        "\n",
        "  gettimeofday(&start, NULL);\n",
        "\n",
        "#ifndef __NVCC__\n",
        "\n",
        "  // Execute the CPU-only reference version\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_ref(ni, nj, tfac, temp1_ref, temp2_ref);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_ref;\n",
        "    temp1_ref = temp2_ref;\n",
        "    temp2_ref= temp_tmp;\n",
        "  }\n",
        "\n",
        "#else\n",
        "\n",
        "  float *temp1_dev, *temp2_dev;\n",
        "\n",
        "  cudaMalloc((void**) &temp1_dev, size);\n",
        "  cudaMalloc((void**) &temp2_dev, size);\n",
        "\n",
        "  cudaMemcpy(temp1_dev, temp1, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(temp2_dev, temp2, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 block_size(BLOCK_WIDTH, BLOCK_HEIGHT);\n",
        "  dim3 grid_size = dim3((nj - 3) / BLOCK_WIDTH + 1, (ni - 3) / BLOCK_HEIGHT + 1);\n",
        "\n",
        "  // Execute the modified version using same data\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_mod<<<grid_size, block_size>>>(ni, nj, tfac, temp1_dev, temp2_dev);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_dev;\n",
        "    temp1_dev = temp2_dev;\n",
        "    temp2_dev = temp_tmp;\n",
        "  }\n",
        "  cudaMemcpy(temp1, temp1_dev, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "#endif\n",
        "\n",
        "  gettimeofday(&stop, NULL);\n",
        "  printf(\"took %lf s\\n\", ((stop.tv_sec - start.tv_sec) * 1000000 + stop.tv_usec - start.tv_usec)/1000000.0);\n",
        "\n",
        "  // float maxError = 0;\n",
        "  // // Output should always be stored in the temp1 and temp1_ref at this point\n",
        "  // for( int i = 0; i < ni*nj; ++i ) {\n",
        "  //   if (abs(temp1[i]-temp1_ref[i]) > maxError) { maxError = abs(temp1[i]-temp1_ref[i]); }\n",
        "  // }\n",
        "  //\n",
        "  // // Check and see if our maxError is greater than an error bound\n",
        "  // if (maxError > 0.0005f)\n",
        "  //   printf(\"Problem! The Max Error of %.5f is NOT within acceptable bounds.\\\\n\", maxError);\n",
        "  // else\n",
        "  //   printf(\"The Max Error of %.5f is within acceptable bounds.\\\\n\", maxError);\n",
        "\n",
        "  free( temp1_ref );\n",
        "  free( temp2_ref );\n",
        "  free( temp1 );\n",
        "  free( temp2 );\n",
        "\n",
        "#ifdef __NVCC__\n",
        "  cudaFree(tmp1_dev);\n",
        "  cudaFree(tmp2_dev);\n",
        "#endif\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "'''\n",
        "with open('assignment-less-dumb.c', 'w') as f:\n",
        "  f.write(code)\n"
      ],
      "metadata": {
        "id": "LQXTlUYXSV6u"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc -O3 -ffast-math assignment-less-dumb.c -o assignment-less-dumb -DSTEPS=200 -DSIZE=13000 && /content/assignment-less-dumb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlZ51uhkdj4X",
        "outputId": "d44f6830-6d95-4513-ad38-e80db8178d97"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 31.249924 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc assignment-less-dumb.cu -o assignment-less-dumb -DSTEPS=200 -DSIZE=13000 -DBLOCK_SIZE=16 && /content/assignment-less-dumb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p18gKfIYfDf2",
        "outputId": "b5952e3c-665a-4cd8-a198-49bf89d528a8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 2.157767 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc assignment-less-dumb.cu -o assignment-less-dumb -DSTEPS=5000 -DSIZE=10000 -DBLOCK_SIZE=16 && /content/assignment-less-dumb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6NKnG1jiSGX",
        "outputId": "236038dd-4c1b-4ea4-9669-ab3f0c0825ed"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 21.750136 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc assignment-less-dumb.cu -o assignment-less-dumb -DSTEPS=5000 -DSHARED -DSIZE=10000 -DBLOCK_SIZE=16 && /content/assignment-less-dumb"
      ],
      "metadata": {
        "id": "lH9R3BvQiN76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e55391-e2db-444e-9bca-38bcf15fbfe3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 32.240333 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import system\n",
        "from subprocess import check_output\n",
        "\n",
        "for i in range(6):\n",
        "  SIZE=3000\n",
        "  #SIZE=100\n",
        "  block_size=2**i\n",
        "  grid_size=(SIZE-3)//block_size+1\n",
        "  #print(f\"{i=} {block_size=}\")\n",
        "  print(f\"{i=} <<({grid_size},{grid_size}),({block_size},{block_size})>>\")\n",
        "  out=check_output(f\"nvcc assignment-less-dumb.cu -o assignment-less-dumb -DSTEPS=5000 -DSHARED -DSIZE={SIZE} -DBLOCK_SIZE={block_size} && /content/assignment-less-dumb\",shell=True,text=True)\n",
        "  print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94C5uB6Qh7hd",
        "outputId": "25fa734e-da88-4fb7-e0b5-fd4bd5b64647"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i=0 <<(2998,2998),(1,1)>>\n",
            "took 62.646371 s\n",
            "\n",
            "i=1 <<(1499,1499),(2,2)>>\n",
            "took 24.665744 s\n",
            "\n",
            "i=2 <<(750,750),(4,4)>>\n",
            "took 7.775655 s\n",
            "\n",
            "i=3 <<(375,375),(8,8)>>\n",
            "took 3.337811 s\n",
            "\n",
            "i=4 <<(188,188),(16,16)>>\n",
            "took 3.193612 s\n",
            "\n",
            "i=5 <<(94,94),(32,32)>>\n",
            "took 2.973721 s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code=r'''#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#ifdef __NVCC__\n",
        "#include <cuda.h>\n",
        "#endif\n",
        "#include <sys/time.h>\n",
        "\n",
        "// Simple define to index into a 1D array from 2D space\n",
        "#define I2D(num, c, r) ((r)*(num)+(c))\n",
        "\n",
        "/*\n",
        " * `step_kernel_mod` is currently a direct copy of the CPU reference solution\n",
        " * `step_kernel_ref` below. Accelerate it to run as a CUDA kernel.\n",
        " */\n",
        "\n",
        "#ifndef BLOCK_SIZE\n",
        "#define BLOCK_SIZE 16\n",
        "#endif\n",
        "#define BLOCK_WIDTH BLOCK_SIZE\n",
        "#define BLOCK_HEIGHT BLOCK_SIZE\n",
        "#ifndef SIZE\n",
        "#define SIZE 1000\n",
        "#endif\n",
        "#ifndef STEPS\n",
        "#define STEPS 200\n",
        "#endif\n",
        "\n",
        "#ifdef __NVCC__\n",
        "#ifndef SHARED\n",
        "#ifdef PITCH\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out, size_t pitch1, size_t pitch2)\n",
        "#else\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "#endif\n",
        "{\n",
        "\n",
        "  /*\n",
        "    COMMENTS: Remember about coalescence (cudaMallocPitch())\n",
        "    remeber balancing between L1 and Shared Mem cudaDeviceSetCacheConfig\n",
        "    __shared__ to access shared memory\n",
        "    __constant__ to set something on constant memory\n",
        "  */\n",
        "  //int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "  int x = threadIdx.x + blockIdx.x * blockDim.x + 1;\n",
        "  int y = threadIdx.y + blockIdx.y * blockDim.y + 1;\n",
        "\n",
        "  if(x >= ni-1 || y >= nj-1) return;\n",
        "\n",
        "  // evaluate derivatives\n",
        "\n",
        "  #ifndef PITCH\n",
        "  d2tdx2 = temp_in[I2D(ni, x - 1, y)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x + 1, y)];\n",
        "  d2tdy2 = temp_in[I2D(ni, x, y - 1)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  // update temperatures\n",
        "  temp_out[I2D(ni, x, y)] = temp_in[I2D(ni, x, y)]+ fact * (d2tdx2 + d2tdy2);\n",
        "  #else\n",
        "  d2tdx2 = temp_in[I2D(pitch1, x - 1, y)] - 2 * temp_in[I2D(pitch1, x, y)] + temp_in[I2D(pitch1, x + 1, y)];\n",
        "  d2tdy2 = temp_in[I2D(pitch1, x, y - 1)] - 2 * temp_in[I2D(pitch1, x, y)] + temp_in[I2D(pitch1, x, y + 1)];\n",
        "\n",
        "  // update temperatures\n",
        "  temp_out[I2D(pitch2, x, y)] = temp_in[I2D(pitch1, x, y)] + fact * (d2tdx2 + d2tdy2);\n",
        "  #endif\n",
        "}\n",
        "#else\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "\n",
        "  /*\n",
        "    COMMENTS: Remember about coalescence (cudaMallocPitch())\n",
        "    remeber balancing between L1 and Shared Mem cudaDeviceSetCacheConfig\n",
        "    __shared__ to access shared memory\n",
        "    __constant__ to set something on constant memory\n",
        "  */\n",
        "  //int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "  __shared__ float sharedMem[(BLOCK_WIDTH + 2) * (BLOCK_HEIGHT + 2)];\n",
        "\n",
        "  int x = threadIdx.x + blockIdx.x * blockDim.x + 1;\n",
        "  int y = threadIdx.y + blockIdx.y * blockDim.y + 1;\n",
        "\n",
        "  if(x >= ni-1 || y >= nj-1) return;\n",
        "\n",
        "  sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)] = temp_in[I2D(ni, x, y)];\n",
        "\n",
        "// TODO: put most of this inside the same warp\n",
        "\n",
        "  if(threadIdx.x == 0) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x, threadIdx.y + 1)] = temp_in[I2D(ni, x - 1, y)];\n",
        "  else if(threadIdx.x == blockDim.x - 1 || x == ni - 2) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 2, threadIdx.y + 1)] = temp_in[I2D(ni, x + 1, y)];\n",
        "\n",
        "  if(threadIdx.y == 0) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y)] = temp_in[I2D(ni, x, y - 1)];\n",
        "  else if(threadIdx.y == blockDim.y - 1 || y == nj - 2) sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 2)] = temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  // evaluate derivatives\n",
        "  d2tdx2 = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x, threadIdx.y + 1)]\n",
        "    - 2 * sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)]\n",
        "    + sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 2, threadIdx.y + 1)];\n",
        "\n",
        "  d2tdy2 = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y)]\n",
        "    - 2 * sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)]\n",
        "    + sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 2)];\n",
        "\n",
        "    //d2tdx2 = temp_in[I2D(ni, x - 1, y)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x + 1, y)];\n",
        "    //d2tdy2 = temp_in[I2D(ni, x, y - 1)] - 2 * temp_in[I2D(ni, x, y)] + temp_in[I2D(ni, x, y + 1)];\n",
        "\n",
        "  // update temperatures\n",
        "  temp_out[I2D(ni, x, y)] = sharedMem[I2D(BLOCK_WIDTH + 2, threadIdx.x + 1, threadIdx.y + 1)] + fact * (d2tdx2 + d2tdy2);\n",
        "\n",
        "}\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  for ( int j=1; j < nj-1; j++ ) {\n",
        "    for ( int i=1; i < ni-1; i++ ) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  struct timeval stop, start;\n",
        "  int istep;\n",
        "  int nstep = STEPS; // number of time steps\n",
        "\n",
        "  // Specify our 2D dimensions\n",
        "  const int ni = SIZE;\n",
        "  const int nj = SIZE;\n",
        "  float tfac = 8.418e-5; // thermal diffusivity of silver\n",
        "\n",
        "  float *temp1_ref, *temp2_ref, *temp1, *temp2, *temp_tmp;\n",
        "\n",
        "  const int size = ni * nj * sizeof(float);\n",
        "\n",
        "  temp1_ref = (float*)malloc(size);\n",
        "  temp2_ref = (float*)malloc(size);\n",
        "  temp1 = (float*)malloc(size);\n",
        "  temp2 = (float*)malloc(size);\n",
        "\n",
        "  // Initialize with random data\n",
        "  for( int i = 0; i < ni*nj; ++i) {\n",
        "    temp1_ref[i] = temp2_ref[i] = temp1[i] = temp2[i] = (float)rand()/(float)(RAND_MAX/100.0f);\n",
        "  }\n",
        "\n",
        "  gettimeofday(&start, NULL);\n",
        "\n",
        "#ifndef __NVCC__\n",
        "\n",
        "  // Execute the CPU-only reference version\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_ref(ni, nj, tfac, temp1_ref, temp2_ref);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_ref;\n",
        "    temp1_ref = temp2_ref;\n",
        "    temp2_ref= temp_tmp;\n",
        "  }\n",
        "\n",
        "#else\n",
        "\n",
        "  float *temp1_dev, *temp2_dev;\n",
        "\n",
        "#ifdef PITCH\n",
        "  size_t pitch1;\n",
        "  cudaMallocPitch((void**) &temp1_dev, &pitch1,nj,ni);\n",
        "  size_t pitch2;\n",
        "  cudaMallocPitch((void**) &temp2_dev, &pitch2,nj,ni);\n",
        "\n",
        "  cudaMemcpy2D(temp1_dev, pitch1,temp1, nj,nj,ni, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy2D(temp2_dev, pitch2,temp2, nj,nj,ni, cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 block_size(BLOCK_WIDTH, BLOCK_HEIGHT);\n",
        "  dim3 grid_size = dim3((nj - 3) / BLOCK_WIDTH + 1, (ni - 3) / BLOCK_HEIGHT + 1);\n",
        "\n",
        "  size_t pitch_tmp;\n",
        "  // Execute the modified version using same data\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_mod<<<grid_size, block_size>>>(ni, nj, tfac, temp1_dev, temp2_dev, pitch1, pitch2);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_dev;\n",
        "    temp1_dev = temp2_dev;\n",
        "    temp2_dev = temp_tmp;\n",
        "\n",
        "    pitch_tmp = pitch1;\n",
        "    pitch1 = pitch2;\n",
        "    pitch2 = pitch_tmp;\n",
        "  }\n",
        "  cudaMemcpy(temp1, temp1_dev, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "#else\n",
        "  cudaMalloc((void**) &temp1_dev, size);\n",
        "  cudaMalloc((void**) &temp2_dev, size);\n",
        "\n",
        "  cudaMemcpy(temp1_dev, temp1, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(temp2_dev, temp2, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 block_size(BLOCK_WIDTH, BLOCK_HEIGHT);\n",
        "  dim3 grid_size = dim3((nj - 3) / BLOCK_WIDTH + 1, (ni - 3) / BLOCK_HEIGHT + 1);\n",
        "\n",
        "  // Execute the modified version using same data\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_mod<<<grid_size, block_size>>>(ni, nj, tfac, temp1_dev, temp2_dev);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_dev;\n",
        "    temp1_dev = temp2_dev;\n",
        "    temp2_dev = temp_tmp;\n",
        "  }\n",
        "  cudaMemcpy(temp1, temp1_dev, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "#endif\n",
        "\n",
        "#endif\n",
        "\n",
        "  gettimeofday(&stop, NULL);\n",
        "  printf(\"took %lf s\\n\", ((stop.tv_sec - start.tv_sec) * 1000000 + stop.tv_usec - start.tv_usec)/1000000.0);\n",
        "\n",
        "  // float maxError = 0;\n",
        "  // // Output should always be stored in the temp1 and temp1_ref at this point\n",
        "  // for( int i = 0; i < ni*nj; ++i ) {\n",
        "  //   if (abs(temp1[i]-temp1_ref[i]) > maxError) { maxError = abs(temp1[i]-temp1_ref[i]); }\n",
        "  // }\n",
        "  //\n",
        "  // // Check and see if our maxError is greater than an error bound\n",
        "  // if (maxError > 0.0005f)\n",
        "  //   printf(\"Problem! The Max Error of %.5f is NOT within acceptable bounds.\\\\n\", maxError);\n",
        "  // else\n",
        "  //   printf(\"The Max Error of %.5f is within acceptable bounds.\\\\n\", maxError);\n",
        "\n",
        "  free( temp1_ref );\n",
        "  free( temp2_ref );\n",
        "  free( temp1 );\n",
        "  free( temp2 );\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "'''\n",
        "with open('assignment-less-dumb.c', 'w') as f:\n",
        "  f.write(code)\n"
      ],
      "metadata": {
        "id": "yVUJLYppgzCx"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc assignment-less-dumb.cu -o assignment-less-dumb -DSTEPS=5000 -DSHARED -DSIZE=10000 -DBLOCK_SIZE=16 -DPITCH && /content/assignment-less-dumb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIhYYKxEg3su",
        "outputId": "fbed16de-cc9a-4a34-b83f-ebdecd54ad58"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 30.195786 s\n"
          ]
        }
      ]
    }
  ]
}